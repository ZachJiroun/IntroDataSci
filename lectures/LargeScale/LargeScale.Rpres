LargeScale
========================================================
author: Hector Corrada Bravo
date: CMSC489T: Intro Data Science

Reminder (Regression)
=======================================================

- _Least squares regression: linear regression, polynomial regression_
- K-nearest neighbors regression
- _Loess_
- Tree-based regression: Regression Trees, Random Forests
- _SVM regression (not covered): linear and non-linear (kernels)_

Reminder (Classification)
======================================================

- _Logistic regression: linear, polynomial_
- LDA, QDA
- K-nearest neighbors classification
- Tree-based classification: Classification Trees, Random Forests
- _SVM classification: linear and non-linear (kernels)_
  
For Today
========================================================

**Question**: How to fit the type of analysis methods we've seen so far for large datasets?

**Insights**: 
  1. All methods in _italics_ were presented as **optimization problems**.
  2. We can devise optimization algorithms that process training data efficiently.

We will use linear regression as a case study of how this insight would work.

Case Study
================================================

Linear regression with one predictor, no interccept

**Given**: Training set $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, with continuous response $y_i$ and predictor $x_i$ for the $i$-th observation.

**Do**: Estimate parameter $\beta_1$ in model $y=\beta_1 x$ to solve

$$
\min_{\beta_1} L(\beta_1) = \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2
$$

Case Study
==============================================

```{r, echo=FALSE, fig.height=10, fig.width=15}
set.seed(1234)
true_beta <- 2
x <- runif(100, -10, 10)
y <- x * true_beta + rnorm(100, mean=0, sd=sqrt(5))
plot(x,y,pch=19,cex=1.4,main="Simulated Data", cex.lab=1.5, cex.main=2)
abline(a=0, b=true_beta, col="red", lwd= 2)
for (b in seq(-3,3, len=5)) {
  abline(a=0,b=b,col="blue", lwd=2, lty=2)
}
legend("bottom", legend=seq(-3,3,len=5), lwd=2, lty=2)
```

Case Study
==============================================

```{r, echo=FALSE, fig.height=10, fig.width=15}
n <- length(y)
compute_error <- function(beta) {
  0.5 * mean((y-x*beta)^2)
}
beta <- seq(-10, 10, len=100)
plot(beta, sapply(beta, compute_error), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]))
abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-3,3,len=5), col="blue", lwd=2, lty=2)
```

Case Study
===============================================

Insights:

1) Loss is minimized when the derivative of the loss function is 0

2) The derivative of the loss (with respect to $\beta_1$ ) at a given estimate $\beta_1$ suggests new values of $\beta_1$ with smaller loss!

$$
\frac{\partial}{\partial \beta{1}} L(\beta_1) = \frac{\partial}{\partial \beta{1}} \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2 \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) \frac{\partial}{\partial \beta_1} (y_i - \beta_1 x_i) \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) (-x_i)
$$

Case Study
================================================

```{r, echo=FALSE, fig.width=15, fig.height=10}
loss_derivative <- function(beta, x, y) {
  f <- beta * x
  resid <- y - f
  sum(resid * (-x))
}

plot(beta, sapply(beta, loss_derivative, x=x, y=y), type="l", lwd=1.5, xlab=expression(beta[1]), ylab=expression(partialdiff * L(beta[1]) / partialdiff * beta[1]),cex.lab=1.7)

abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-3,3,len=5), col="blue", lwd=2, lty=2)
```

Case Study
=================================================

This suggests an algorithm:

1. Initialize $\beta_1=0$
2. While $|\frac{\partial L(\beta_1)}{\beta_1}| > \mathrm{tol}$
  - Set $\beta_1 = \beta_1 + -\alpha \frac{\partial L(\beta_1)}{\beta_1}$

This algorithm is called **gradient descent** in the general case.

Case Study
=============

```{r}
gradient_descent <- function(x, y, tol=1e-6, plot=FALSE) {
  beta_1 <- 0; i <- 0
  deriv <- loss_derivative(beta_1, x, y)
  alpha <- 1e-3
  while ((abs(deriv) > tol) && (i < 30)) {
    cat("it: ", i, " beta: ", round(beta_1, 2), " deriv: ", round(deriv,6), " alpha: ", round(alpha, 6), "\n")
    if (plot) {
      abline(v=beta_1)
      
    }
    beta_1 <- beta_1 - alpha * deriv
    deriv <- loss_derivative(beta_1, x, y)
    i <- i+1
    if ((i %% 5) == 0) alpha <- alpha / 2
  }
  if (plot) abline(v=beta_1)
  beta_1
}
```

Case Study
===============

```{r, echo=FALSE}
plot(beta, sapply(beta, compute_error), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]))
estimate <- gradient_descent(x, y, plot=TRUE)
```

