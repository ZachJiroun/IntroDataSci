LargeScale
========================================================
author: Hector Corrada Bravo
date: CMSC489T: Intro Data Science

```{r, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

Reminder (Regression)
=======================================================

- _Least squares regression: linear regression, polynomial regression_
- K-nearest neighbors regression
- _Loess_
- Tree-based regression: Regression Trees, Random Forests
- _SVM regression (not covered): linear and non-linear (kernels)_

Reminder (Classification)
======================================================

- _Logistic regression: linear, polynomial_
- LDA, QDA
- K-nearest neighbors classification
- Tree-based classification: Classification Trees, Random Forests
- _SVM classification: linear and non-linear (kernels)_
  
For Today
========================================================

**Question**: How to fit the type of analysis methods we've seen so far for large datasets?

**Insights**: 
  1. All methods in _italics_ were presented as **optimization problems**.
  2. We can devise optimization algorithms that process training data efficiently.

We will use linear regression as a case study of how this insight would work.

Case Study
================================================

Linear regression with one predictor, no interccept

**Given**: Training set $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, with continuous response $y_i$ and predictor $x_i$ for the $i$-th observation.

**Do**: Estimate parameter $\beta_1$ in model $y=\beta_1 x$ to solve

$$
\min_{\beta_1} L(\beta_1) = \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2
$$

Case Study
==============================================

```{r, echo=FALSE, fig.height=10, fig.width=15}
set.seed(1234)
true_beta <- 2
x <- runif(100, -10, 10)
y <- x * true_beta + rnorm(100, mean=0, sd=sqrt(5))
plot(x,y,pch=19,cex=1.4,main="Simulated Data", cex.lab=1.5, cex.main=2)
abline(a=0, b=true_beta, col="red", lwd= 2)
for (b in seq(-3,3, len=5)) {
  abline(a=0,b=b,col="blue", lwd=2, lty=2)
}
legend("bottom", legend=seq(-3,3,len=5), lwd=2, lty=2)
```

Case Study
==============================================

```{r, echo=FALSE, fig.height=10, fig.width=15}
n <- length(y)
compute_loss <- function(beta, x, y) {
  0.5 * mean((y-x*beta)^2)
}
beta <- seq(-10, 10, len=100)
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]))
abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-3,3,len=5), col="blue", lwd=2, lty=2)
```

Case Study
===============================================

Insights:

1) Loss is minimized when the derivative of the loss function is 0

2) The derivative of the loss (with respect to $\beta_1$ ) at a given estimate $\beta_1$ suggests new values of $\beta_1$ with smaller loss!

$$
\frac{\partial}{\partial \beta{1}} L(\beta_1) = \frac{\partial}{\partial \beta{1}} \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2 \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) \frac{\partial}{\partial \beta_1} (y_i - \beta_1 x_i) \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) (-x_i)
$$

Case Study
================================================

```{r, echo=FALSE, fig.width=15, fig.height=10}
loss_derivative <- function(beta, x, y) {
  f <- beta * x
  resid <- y - f
  sum(resid * (-x))
}

plot(beta, sapply(beta, loss_derivative, x=x, y=y), type="l", lwd=1.5, xlab=expression(beta[1]), ylab=expression(partialdiff * L(beta[1]) / partialdiff * beta[1]),cex.lab=1.7)

abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-3,3,len=5), col="blue", lwd=2, lty=2)
```

Gradient Descent
=================================================

This suggests an algorithm:

1. Initialize $\beta_1=0$
2. Repeat until convergence
  - Set $\beta_1 = \beta_1 + \alpha \sum_{i=1}^n (y_i - f(x_i)) x_i$
  
This algorithm is called **gradient descent** in the general case.

Gradient Descent
=================

```{r, echo=FALSE}
gradient_descent <- function(x, y, tol=1e-6, maxit=50, plot=FALSE) {
  beta_1 <- 0; old_beta_1 <- Inf; i <- 0; beta_keep <- NA
  deriv <- loss_derivative(beta_1, x, y);
  loss <- compute_loss(beta_1, x, y); loss_keep <- NA
  
  alpha <- 1e-3
  difference <- (beta_1 - old_beta_1)^2 / (old_beta_1)^2
  
  while ((difference > tol) && (i < maxit)) {
    cat("it: ", i, " beta: ", round(beta_1, 2), "loss: ", round(loss, 2), " deriv: ", round(deriv,6), " alpha: ", round(alpha, 6), "\n")
    if (plot && !is.na(beta_keep) && !is.na(loss_keep)) {
      suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
    }
    beta_keep <- beta_1; loss_keep <- loss;
    old_beta_1 <- beta_1
    beta_1 <- beta_1 - alpha * deriv
    difference <- (beta_1 - old_beta_1)^2 / (old_beta_1)^2
  
    loss <- compute_loss(beta_1, x, y)
    deriv <- loss_derivative(beta_1, x, y)
    i <- i+1
    if ((i %% 5) == 0) alpha <- alpha / 2
  }
  if (plot) {
    suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
  }
  beta_1
}
```

Gradient Descent
===============

```{r, echo=FALSE}
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]), xlim=c(-10,10))
estimate <- gradient_descent(x, y, plot=TRUE)
```

Gradient Descent
=================

```{r, echo=FALSE}
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]), xlim=c(-3,3))
estimate <- gradient_descent(x, y, plot=TRUE)
```

Gradient Descent
==================

This is referred to as "Batch" gradient descent, since we take a step (update $\beta_1$) by calculating derivative with respect to all $n$ observations.

Let's write out the update equation:

$$
\beta_1 = \beta_1 + \alpha \sum_{i=1}^n (y_i - f(x_i, \beta_1)) x_i
$$

where $f(x_i) = \beta_1 x_i$.

Gradient Descent
==================

Note: For multiple predictors (e.g., adding an intercept), this generalizes to the _gradient_ i.e., the vector of first derivatives of _loss_ with respect to parameters.

The update equation is exactly the same for least squares regression

$$
\beta = \beta + \alpha \sum_{i=1^n} (y_i - f(\mathbf{x}_i, \beta)) \mathbf{x}_i
$$

First-order methods
============================

Gradiest descent falls within a family of optimization methods called _first-order methods_.

These methods have properties amenable to use with very large datasets:

1. Inexpensive updates    
2. "Stochastic" version can converge with few sweeps of the data  
3. "Stochastic" version easily extended to streams  
4. Easily parallelizable  

Drawback: Takes many steps to converge

Stochastic gradient descent
============================

**Key Idea**: Update parameters using update equation _one observation at a time_:

1. Initialize $\beta=\mathbf{0}$, $i=1$
2. Repeat until convergence
  - For $i=1$ to $n$
    - Set $\beta = \beta + \alpha (y_i - f(\mathbf{x}_i, \beta)) \mathbf{x}_i$

```{r, echo=FALSE}
stochastic_gradient_descent <- function(x, y, tol=1e-6, maxit=50, plot=FALSE) {
  n <- length(y)
  
  beta_1 <- 0; i <- 0; beta_keep <- NA
  deriv <- loss_derivative(beta_1, x, y);
  loss <- compute_loss(beta_1, x, y); loss_keep <- NA
  
  alpha <- 1e-3
  difference <- (beta_1 - old_beta_1)^2 / (old_beta_1)^2
  
  while ((difference > tol) && (i < maxit)) {
    cat("it: ", i, " beta: ", round(beta_1, 2), "loss: ", round(loss, 2), " deriv: ", round(deriv,6), " alpha: ", round(alpha, 6), "\n")
    
    for (j in seq(1,n)) {
      if (plot && !is.na(beta_keep) && !is.na(loss_keep)) {
        suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
      }
      
      beta_keep <- beta_1; loss_keep <- loss;
      
      f <- beta_1 * x[j]
      resid <- y[j] - f
      
      beta_1 <- beta_1 + alpha * resid * x[j]
      loss <- compute_loss(beta_1, x, y)
    }
    
    deriv <- loss_derivative(beta_1, x, y)
    i <- i+1
    if ((i %% 5) == 0) alpha <- alpha / 2
  }
  
  if (plot) {
    suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
  }
  
  beta_1
}
```

Stochastic Gradient Descent
============================

```{r, echo=FALSE}
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]), xlim=c(-3,3))
estimate <- stochastic_gradient_descent(x, y, plot=TRUE)
```

Stochastic Gradient Descent
=============================

Easily adapt to _data streams_ where we receive observations one at a time and _assume_ they are not stored.

**Idea**: Update parameter using same update rule
_Note_: This falls in the general category of _online_ learning.

Gradient Descent
==================

Easily parallelizable:

Split observations 